# -*- coding: utf-8 -*-
"""Collecting Data From ERA5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_Gap79GwXnHnBfRhaV5MIJydvcYpbfc
"""

!pip install cdsapi cfgrib xarray pandas netcdf4 h5netcdf

# Commented out IPython magic to ensure Python compatibility.
# # Ensure colab doesn't disconnect
# %%javascript
# function ClickConnect(){
# console.log("Working");
# document.querySelector("colab-toolbar-button#connect").click()
# }setInterval(ClickConnect,60000)

import cdsapi
import os
import zipfile
import xarray as xr
import pandas as pd
import time
import glob
import numpy as np

pd.options.display.max_columns = None
pd.options.display.max_rows = None
pd.options.display.date_dayfirst = True
pd.options.display.float_format = '{:.2f}'.format

with open("/root/.cdsapirc", "w") as f:
    # Replace with your API configuration
    f.write("url: https://cds.climate.copernicus.eu/api\n")
    f.write("key: your own key\n")

c = cdsapi.Client()

# Target variables
variables = [
    '2m_temperature', 'surface_pressure', '2m_dewpoint_temperature',
    '10m_u_component_of_wind', '10m_v_component_of_wind',
    'surface_solar_radiation_downwards', 'total_precipitation'
]

# Target location # N,W,S,E
area = [30, 31.00, 30.05, 31.01] # Our specific location
# area = [31.6, 24.6, 21.7, 37] # Whole Egypt

# Years and quarters (3 months per request)
# As for single point, the maximum request was 3 months
# years = list(range(2020, 2025))
years = [2021] # To download single year
quarters = [(1,3), (4,6), (7,9), (10,12)]

os.makedirs("downloads", exist_ok=True)
all_dfs = []

for year in years:
    for i, (start_month, end_month) in enumerate(quarters):
        months = [f"{m:02d}" for m in range(start_month, end_month+1)]

        os.makedirs(f"downloads/{i}", exist_ok=True)
        zip_filename = f"downloads/{i}/era5_{year}_{months[0]}-{months[-1]}.zip"
        nc_filename = f"downloads/{i}/era5_{year}_{months[0]}-{months[-1]}.nc"

        print(f"Downloading {zip_filename}...")

        # Wait 60seconds before each request
        time.sleep(60)
        c.retrieve(
            'reanalysis-era5-single-levels',
            {
                'product_type': 'reanalysis',
                'variable': variables,
                'year': str(year),
                'month': months,
                'day': [f"{d:02d}" for d in range(1, 32)],
                'time': [f"{h:02d}:00" for h in range(24)],
                'area': area,
                "data_format": "netcdf",
                "download_format": "unarchived"
            },
            zip_filename
        )

        # Extract .zip
        print(f"Extracting {zip_filename}...")
        with zipfile.ZipFile(zip_filename, "r") as zip_ref:
            zip_ref.extractall(f"downloads/{i}")

        # Open each file as a dataset and collect all data from each request
        nc_files = sorted(glob.glob(f"downloads/{i}/*.nc"))
        datasets = [xr.open_dataset(f) for f in nc_files]

        # Drop 'expver' & 'number' as only one value
        datasets = [ds.drop_vars(['expver', 'number'], errors='ignore') for ds in datasets]

        # Merge all datasets into one based on time, lat, lon
        merged_ds = xr.merge(datasets, join='inner')

        df = merged_ds.to_dataframe().reset_index()
        all_dfs.append(df)

# Collect in single CSV file
print("Merging all data into one CSV")
df = pd.concat(all_dfs, ignore_index=True)

df.to_csv("era5_weather_all.csv", index=False)
print("Finished.")

### Calculating the required features

# Temperature from Kelvin to Celsius
df['temperature_C'] = df['t2m'] - 273.15

# Surface pressure form Pa to hPa
df['pressure_hPa'] = df['sp'] / 100

# Relative Humidity (%)
df['Td'] = df['d2m'] - 273.15
df['humidity_%'] = 100 * (np.exp((17.625 * df['Td']) / (243.04 + df['Td'])) /
            np.exp((17.625 * df['temp_c']) / (243.04 + df['temp_c'])))

# Wind speed & direction
df['wind_speed_mps'] = np.sqrt(df['u10']**2 + df['v10']**2)
df['wind_dir_deg'] = (np.arctan2(-df['u10'], -df['v10']) * (180 / np.pi) + 360) % 360

# Solar radiation from J/m² to W/m², divide by time internal (1h = 3600s)
df['solar_radiation_Wm2'] = (df['ssrd']/3600)

# Total precipitation from m to mm/h, hourly data means it's already per hour
df['rain_mm_h'] = (df['tp'] * 1000)

# df = df[['valid_time', 'latitude', 'longitude', 'temperature_C', 'pressure_hPa', 'humidity_%', 'wind_speed_mps', 'wind_dir_deg', 'solar_radiation_Wm2', 'rain_mm_h']]
df = df.drop(columns=['Td', 't2m', 'sp', 'd2m', 'u10', 'v10', 'ssrd', 'tp'])

df.to_csv("CairoPoint_2021_weather.csv", index=False)
print("Done")



"""## Some Notes
Multiple readings for the same timestamp, if using whole Egypt coordinates

ERA5 uses a 0.25° x 0.25° grid\
Latitudes: 21.0°N → 31.0°N (inclusive) → ((31 - 21) / 0.25) + 1 = 41 points\
Longitudes: 24.0°E → 37.0°E (inclusive) → ((37 - 24) / 0.25) + 1 = 53 points\
Total grid points: 41 × 53 = 2,173

Around 2000 records for each time stamp.

These 7 features are stored on the cloud in 2 separate places based on they are instant or accumulated features. So they are downloaded as 2 .nc files collected in zip file.\
We first extract them form the zip file and merge the 2 files based on the time stamp, latitude, and longitude.
"""



